{"cells":[{"cell_type":"code","source":["# Set arguments\n","PrimaryKeys = \"\"\n","IsIncremental = False\n","\n","SourceWorkspace= \"\"\n","SourceLakehouse = \"\"\n","SourceLakehouseName = \"\"\n","source_file_path = \"\"\n","source_file_name = \"\"\n","source_file_type = \"\"\n","\n","TargetWorkspace = \"\"\n","TargetLakehouse = \"\"\n","TargetLakehouseName = \"\"\n","target_schema = \"\"\n","target_name = \"\"\n","\n","\n","# # CSV\n","CompressionType = 'infer'\n","ColumnDelimiter = ','\n","RowDelimiter = '\\n'\n","EscapeCharacter = '\"'\n","Encoding = 'UTF-8'\n","first_row_is_header = True\n","infer_schema = True\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c06e7bda-944e-4f48-a910-5921a853e130"},{"cell_type":"markdown","source":["## Load Libraries"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"31dcb328-59f4-4860-835b-beb9827829d3"},{"cell_type":"code","source":["import re\n","import datetime\n","import json\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from delta.tables import *\n","from notebookutils import mssparkutils\n","import uuid"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"32597cb6-429a-485f-9793-472084ca2284"},{"cell_type":"markdown","source":["## Define Starttime"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a4c5b7d4-0868-4c10-b839-a6766a0ac1d3"},{"cell_type":"code","source":["start_audit_time = datetime.datetime.now()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"29d1c022-0a73-45ce-bd7c-24126209a444"},{"cell_type":"markdown","source":["## Set Configuration"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f42a820a-1456-47ed-8511-cbd9e19756e8"},{"cell_type":"code","source":["#Make sure you have enabled V-Order\n","\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ae466f6-86c1-4bf0-838e-bcf0f3d3f7d0"},{"cell_type":"markdown","source":["## Set your loading paths"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"aaa2def1-1e2f-475b-98da-cdf3431e4568"},{"cell_type":"code","source":["#Set SourceFile and target Location\n","source_changes_data_path = f\"abfss://{SourceWorkspace}@onelake.dfs.fabric.microsoft.com/{SourceLakehouse}/Files/{source_file_path}/{source_file_name}\"\n","print(source_changes_data_path)\n","\n","#Beware \n","target_data_path = f\"abfss://{TargetWorkspace}@onelake.dfs.fabric.microsoft.com/{TargetLakehouse}/Tables/{target_schema}{target_name}\"\n","print(target_data_path)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7902d60f-0dd4-4987-a3ba-99a877ed2ffc"},{"cell_type":"markdown","source":["## Load new from Data Landingzone"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7162aed6-1e70-4b24-9f7c-448ff1aa7794"},{"cell_type":"code","source":["#Read all incoming changes in Parquet format\n","dfDataChanged= spark.read\\\n","                .format(source_file_type) \\\n","                .option(\"header\",\"true\") \\\n","                .load(f\"{source_changes_data_path}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7b75859c-999c-4a09-b89c-2534a7b8b35e"},{"cell_type":"code","source":["# Replace spaces with underscores in column names\n","new_columns = [col.replace(' ', '') for col in dfDataChanged.columns]\n","\n","# Rename the columns\n","dfDataChanged = dfDataChanged.toDF(*new_columns)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd1b71ed-fbde-403b-989c-5193ab894193"},{"cell_type":"markdown","source":["## DQ Checks"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0876c3a1-0065-4005-b866-60ec5380d512"},{"cell_type":"code","source":["#split PKcolumns string on , ; or :\n","PrimaryKeys = str(PrimaryKeys)\n","\n","PrimaryKeys = re.split('[, ; :]', PrimaryKeys)\n","#remove potential whitespaces around Pk columns\n","PrimaryKeys = [column.strip() for column in PrimaryKeys if column != \"\"]\n","\n","key_columns = PrimaryKeys\n","print(f\": {', '.join(key_columns)}\")\n","# Check if all PK's exist in source\n","for pk_column in key_columns:\n","    if pk_column not in dfDataChanged.columns:\n","        raise ValueError(f\"PK: {pk_column} doesn't exist in the source.\")\n","        # Define all the Non-Key columns => HashExcludeColumns\n","\n","read_key_columns = [column for column in dfDataChanged.columns if column in key_columns]\n","\n","# Add a column with the calculated hash, easier in later stage of with multiple PK\n","dfDataChanged = (dfDataChanged\n","                .withColumn(\"HashedPKColumn\", sha2(concat_ws(\"||\", *read_key_columns), 256)))\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"79e03490-11b8-491c-bb9b-c0991e2696da"},{"cell_type":"markdown","source":["## Check for Duplicates"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bbbbe331-2591-49d6-a22a-271606fc3e56"},{"cell_type":"code","source":["if dfDataChanged.select('HashedPKColumn').distinct().count() != dfDataChanged.select('HashedPKColumn').count():\n","    raise ValueError(f'Source file contains duplicated rows for PK: {\", \".join(key_columns)}')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bbaa0e3a-000e-4312-b42c-46254384bc65"},{"cell_type":"markdown","source":["## Add Hash"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"30c34b0c-9c98-4742-b245-f07fdb65d43d"},{"cell_type":"code","source":["non_key_columns = [column for column in dfDataChanged.columns if column not in key_columns]\n","\n","#add a hashed cloumn to detect changes\n","dfDataChanged = (dfDataChanged\n","                .withColumn(\"HashedNonKeyColumns\", md5(concat_ws(\"||\", *non_key_columns))))\n","\n","#Add RecordLoadDate to see when the record arrived\n","dfDataChanged = dfDataChanged.withColumn('RecordLoadDate', current_timestamp())\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26daaffe-27ca-439a-8a2d-a7bc15839fa3"},{"cell_type":"markdown","source":["## Read Original if exists"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d1ed3be6-59cd-4f0d-99eb-72f8017c0447"},{"cell_type":"code","source":["#Check if Target exist, if exists read the original data if not create table and exit\n","if DeltaTable.isDeltaTable(spark, target_data_path):\n","    # Read original/current data\n","    dfDataOriginal = (spark\n","                        .read.format(\"delta\")\n","                        .load(target_data_path)\n","                        )\n","\n","else:\n","    # Use first load when no data exists yet and then exit \n","    dfDataChanged.write.format(\"delta\").mode(\"overwrite\").save(target_data_path)\n","    TotalRuntime = str((datetime.datetime.now() - start_audit_time)) \n","\n","    # Your data\n","    result_data = {\n","        \"CopyOutput\":{\n","            \"Total Runtime\": TotalRuntime,\n","            \"TargetSchema\": target_schema,\n","            \"TargetName\" : target_name\n","        }\n","        }\n","\n","\n","    mssparkutils.notebook.exit(result_data)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fb27b6e7-7bfb-4d19-ab6b-ecc8697eb3b1"},{"cell_type":"markdown","source":["## Merge table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d9acbca6-ee61-47a9-8a89-036669c503b0"},{"cell_type":"code","source":["#merge table \n","deltaTable = DeltaTable.forPath(spark, f'{target_data_path}')\n","if IsIncremental in [False, 'false', 'False']:\n","    print(' - Incremental Loading is not enabled, deletes are allowed')\n","    merge = deltaTable.alias('original') \\\n","        .merge(dfDataChanged.alias('updates'), 'original.HashedPKColumn == updates.HashedPKColumn') \\\n","        .whenNotMatchedInsertAll() \\\n","        .whenMatchedUpdateAll('original.HashedNonKeyColumns != updates.HashedNonKeyColumns') \\\n","        .whenNotMatchedBySourceDelete() \\\n","        .execute()\n","elif IsIncremental not in [False, 'false', 'False']:\n","    print(' - Incremental Loading is enabled, deletes are not allowed')\n","    merge = deltaTable.alias('original') \\\n","        .merge(dfDataChanged.alias('updates'), 'original.HashedPKColumn == updates.HashedPKColumn') \\\n","        .whenNotMatchedInsertAll() \\\n","        .whenMatchedUpdateAll('original.HashedNonKeyColumns != updates.HashedNonKeyColumns') \\\n","        .execute()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8e228eee-4205-44a9-af62-6df6b3c485a0"},{"cell_type":"markdown","source":["## Exit notebook"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"89624627-7fc9-4ffe-ab4d-1f562c7a912d"},{"cell_type":"code","source":["TotalRuntime = str((datetime.datetime.now() - start_audit_time)) \n","\n","# Your data\n","result_data = {\n","    \"CopyOutput\":{\n","        \"Total Runtime\": TotalRuntime,\n","        \"TargetSchema\": target_schema,\n","        \"TargetName\" : target_name\n","    }\n","    }\n","\n","\n","mssparkutils.notebook.exit(result_data)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c8f8687c-7d0f-4ce7-a07b-8014d8d892fe"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"kernel_info":{"name":"synapse_pyspark"},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"009058ac-b71c-4774-a8ee-7c7d945c3972"},{"id":"e130dba7-c8c3-438a-85ad-2cd4c9d59a09"}],"default_lakehouse":"009058ac-b71c-4774-a8ee-7c7d945c3972","default_lakehouse_name":"LH_Data_Landingzone","default_lakehouse_workspace_id":"586fc19d-fa6a-4cb1-9ca3-e518a524f5da"}}},"nbformat":4,"nbformat_minor":5}